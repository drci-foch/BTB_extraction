{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/60lgrf9s6sqbf6lcr70gbl2c0000gn/T/ipykernel_91863/3862809403.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with pdf - obsolete \n",
    "Way too long !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_data_in_text(text):\n",
    "\n",
    "    # Multiline regex patterns for Nom, Prénom, Date de Naissance, and Sexe\n",
    "    nom_pattern = r\"(?i)NOM\\s*:\\s*([A-Z]+)\"\n",
    "    prenom_pattern = r\"Prénom\\s*:\\s*([A-Z\\s]+)\"\n",
    "\n",
    "    date_naissance_pattern = r\"Date de naissance\\s*:\\s*(\\d{2}/\\d{2}/\\d{4})\"\n",
    "    sexe_pattern = r\"Sexe\\s*:\\s*([MF])\"\n",
    "\n",
    "    # Using re.DOTALL to ensure dot matches across newlines\n",
    "    nom_match = re.search(nom_pattern, text, re.DOTALL)\n",
    "    prenom_match = re.search(prenom_pattern, text, re.DOTALL)\n",
    "    date_naissance_match = re.search(date_naissance_pattern, text, re.DOTALL)\n",
    "    sexe_match = re.search(sexe_pattern, text, re.DOTALL)\n",
    "\n",
    "    # Extracting matched groups, handling None\n",
    "    nom = nom_match.group(1).strip() if nom_match else None\n",
    "    prenom = prenom_match.group(1).strip() if prenom_match else None\n",
    "    date_naissance = date_naissance_match.group(\n",
    "        1) if date_naissance_match else None\n",
    "    sexe = sexe_match.group(1) if sexe_match else None\n",
    "\n",
    "    return nom, prenom, date_naissance, sexe\n",
    "\n",
    "\n",
    "def process_pdf_files_in_directory(directory_path):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            # Extract IPP from the filename\n",
    "            ipp = filename[:9]\n",
    "\n",
    "            pdf_path = os.path.join(directory_path, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            # Extract required information using the updated function\n",
    "            nom, prenom, date_naissance, sexe = find_data_in_text(text)\n",
    "\n",
    "            # Append the extracted data including IPP\n",
    "            data.append([filename, ipp, nom, prenom, date_naissance, sexe])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        data, columns=[\"Filename\", \"IPP\", \"Nom\",\n",
    "                       \"Prénom\", \"Date de Naissance\", \"Sexe\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"./data/data_extractMoustapha_2020:2021\"\n",
    "df = process_pdf_files_in_directory(directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going for txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prenom_before_docteur(text):\n",
    "    pattern = r\"Prénom\\s*:\\s*([A-Z\\s]+)\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        # Extract the captured group before \"Docteur\"\n",
    "        before_docteur = match.group(1)\n",
    "        first_name = before_docteur.split()[0] if before_docteur else None\n",
    "        return first_name\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_nom(text):\n",
    "    nom_pattern = r\"(?i)(^Nom\\s*:\\s*|Nom[\\s+]*usuel\\s*:\\s*|Nom\\s*:\\s*)([A-Z]+)\"\n",
    "    nom_match = re.search(nom_pattern, text, re.DOTALL)\n",
    "    return nom_match.group(2).strip() if nom_match else None\n",
    "\n",
    "\n",
    "def extract_ddn(text):\n",
    "    date_naissance_pattern = (\n",
    "        r\"(?i)(Date[\\s+]*de[\\s+]*naissance\\s*:\\s*)(\\d{2}/\\d{2}/\\d{4})\"\n",
    "    )\n",
    "    date_naissance_match = re.search(date_naissance_pattern, text, re.DOTALL)\n",
    "    return date_naissance_match.group(2).strip() if date_naissance_match else None\n",
    "\n",
    "\n",
    "def extract_sex(text):\n",
    "    sexe_pattern = r\"Sexe\\s*:\\s*([MF])\"\n",
    "    sexe_match = re.search(sexe_pattern, text, re.DOTALL)\n",
    "    return sexe_match.group(1) if sexe_match else None\n",
    "\n",
    "\n",
    "def extract_ddprelevement(text):\n",
    "    date_prelevement_pattern = (\n",
    "        r\"(?i)(Prélevé le \\s*:\\s*|Prélevé[\\s+]*le\\s*:\\s*)(\\d{2}/\\d{2}/\\d{4})\"\n",
    "    )\n",
    "    date_prelevement_match = re.search(date_prelevement_pattern, text, re.DOTALL)\n",
    "    return date_prelevement_match.group(2) if date_prelevement_match else None\n",
    "\n",
    "\n",
    "def extract_technique(text):\n",
    "    # Most found pattern\n",
    "    technique_pattern = r\"(2\\.|II\\.|I\\.|2/|2°/)[\\s+]*(Biopsies\\s+trans[ -]*bronchiques|Biopsies\\s+transbronchiques|Biospies\\s+transbronchiques|BTB)[\\s\\S+]*?Technique[^\\S\\r\\n]*:[^\\S\\r\\n]*([^;]+)\"\n",
    "    technique_match = re.search(technique_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    if not technique_match:\n",
    "        # Sometimes, there isn't the numbers, only \"Biopsie\" in text etc..\n",
    "        fallback_technique_pattern = r\"(Biopsies\\s+trans[ -]*bronchiques|Biopsies\\s+transbronchiques|Biospies\\s+transbronchiques|BTB)[\\s\\S+]*?Technique\\s*:\\s*([^;]+)\"\n",
    "        fallback_match = re.search(fallback_technique_pattern, text, re.DOTALL)\n",
    "        if fallback_match:\n",
    "            return fallback_match.group(2).strip()\n",
    "\n",
    "        else:\n",
    "            # No mention of biopsie or lavage, just the mention of techniques two times. The BTB technique is always second.\n",
    "            parts = re.split(r\"(?=Technique\\s*:)\", text, flags=re.IGNORECASE)\n",
    "            two_parts_fallback_technique_pattern = r\"Technique\\s*:\\s*([^;]+)\"\n",
    "            if len(parts) > 2:\n",
    "                two_parts_fallback_match = re.search(\n",
    "                    two_parts_fallback_technique_pattern, parts[-1], re.DOTALL\n",
    "                )\n",
    "                if two_parts_fallback_match:\n",
    "                    return two_parts_fallback_match.group(1).strip()\n",
    "\n",
    "            # Else, look at any mention of technique in the document\n",
    "            else:\n",
    "                last_fallback_technique_pattern = r\"Technique\\s*:\\s*([^;]+)\"\n",
    "                last_fallback_match = re.search(\n",
    "                    last_fallback_technique_pattern, text, re.DOTALL\n",
    "                )\n",
    "                if last_fallback_match:\n",
    "                    return last_fallback_match.group(1).strip()\n",
    "\n",
    "    return technique_match.group(3).strip() if technique_match else None\n",
    "\n",
    "\n",
    "def extract_niveaux_coupes(text):\n",
    "    \"\"\"\n",
    "    In this function we just reuse the extract_technique patterns to extract the group after the semi-colon\n",
    "    \"\"\"\n",
    "    # Most found pattern\n",
    "    technique_pattern = r\"(2\\.|II\\.|I\\.|2/|2°/)[\\s+]*(Biopsies\\s+trans[ -]*bronchiques|Biopsies\\s+transbronchiques|Biospies\\s+transbronchiques|BTB)[\\s\\S+]*?Technique\\s*:\\s*([^;]+);\\s*([^n]+)\"\n",
    "    technique_match = re.search(technique_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    if not technique_match:\n",
    "        # Sometimes, there isn't the numbers, only \"Biopsie\" in text etc..\n",
    "        fallback_technique_pattern = r\"(Biopsies\\s+trans[ -]*bronchiques|Biopsies\\s+transbronchiques|Biospies\\s+transbronchiques|BTB)[\\s\\S+]*?Technique\\s*:\\s*([^;]+);\\s*([^n]+)\"\n",
    "        fallback_match = re.search(fallback_technique_pattern, text, re.DOTALL)\n",
    "        if fallback_match:\n",
    "            return fallback_match.group(3).strip()\n",
    "\n",
    "        else:\n",
    "            # No mention of biopsie or lavage, just the mention of techniques two times. The BTB technique is always second.\n",
    "            parts = re.split(r\"(?=Technique\\s*:)\", text, flags=re.IGNORECASE)\n",
    "            two_parts_fallback_technique_pattern = r\"Technique\\s*:\\s*([^;]+);\\s*([^n]+)\"\n",
    "            if len(parts) > 2:\n",
    "                two_parts_fallback_match = re.search(\n",
    "                    two_parts_fallback_technique_pattern, parts[-1], re.DOTALL\n",
    "                )\n",
    "                if two_parts_fallback_match:\n",
    "                    return two_parts_fallback_match.group(2).strip()\n",
    "\n",
    "            # Else, look at any mention of technique in the document\n",
    "            else:\n",
    "                last_fallback_technique_pattern = r\"Technique\\s*:\\s*([^;]+);\\s*([^n]+)\"\n",
    "                last_fallback_match = re.search(\n",
    "                    last_fallback_technique_pattern, text, re.DOTALL\n",
    "                )\n",
    "                if last_fallback_match:\n",
    "                    return last_fallback_match.group(2).strip()\n",
    "\n",
    "    return technique_match.group(4).strip() if technique_match else None\n",
    "\n",
    "\n",
    "def extract_site(text):\n",
    "    site_pattern = r\"(Site[\\s\\xa0]*:)([\\S]*[^\\n]+)\"\n",
    "    site_match = re.search(site_pattern, text, re.DOTALL)\n",
    "    return site_match.group(2).strip() if site_match else None\n",
    "\n",
    "\n",
    "def extract_fragment_alveolaire(text):\n",
    "    frag_alveolaire_pattern = r\"(Nombre[\\s\\xa0]*de[\\s\\xa0]*fragments[\\s\\xa0]*alvéolaires[\\s\\xa0]*:)([\\S]*[^\\n]+)\"\n",
    "    frag_alveolaire_match = re.search(frag_alveolaire_pattern, text, re.DOTALL)\n",
    "    return frag_alveolaire_match.group(2).strip() if frag_alveolaire_match else None\n",
    "\n",
    "\n",
    "def extract_bronche_bronchiole(text):\n",
    "    bronche_bronchiole_pattern = r\"(Bronches\\/Bronchioles[\\s\\xa0]*:[\\s\\xa0]*|Bronches[\\s\\xa0]*\\/[\\s\\xa0]*Bronchioles[\\s\\xa0]*:[\\s\\xa0]*)([\\S]*[^\\n]+)\"\n",
    "    bronche_bronchiole_match = re.search(bronche_bronchiole_pattern, text, re.DOTALL)\n",
    "    return (\n",
    "        bronche_bronchiole_match.group(2).strip() if bronche_bronchiole_match else None\n",
    "    )\n",
    "\n",
    "def extract_infiltrat(text):\n",
    "    infiltrat_pattern = r\"(Infiltrat[\\s\\xa0]*mononucléé[\\s\\xa0]*péri(?:-|\\s*)?vasculaire[\\s\\xa0]*\\(A0[\\s\\xa0]*à[\\s\\xa0]*A4[\\s\\xa0]*\\/[\\s\\xa0]*AX\\)[\\s\\xa0]*:*:[\\s\\xa0]*|Infiltrat[\\s\\xa0]*mononucléé[\\s\\xa0]*péri(?:-|\\s*)?vasculaire[\\s\\xa0]*\\(A[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n]+)\"\n",
    "    infiltrat__match = re.search(infiltrat_pattern, text, re.DOTALL)\n",
    "    return infiltrat__match.group(2).strip() if infiltrat__match else None\n",
    "\n",
    "\n",
    "def extract_bronchiolite_lymph(text):\n",
    "    bronchlymph_pattern = r\"(Bronchiolite[\\s\\xa0]*lymphocytaire[\\s\\xa0]*\\(B0[\\s\\xa0]*\\/[\\s\\xa0]*1R[\\s\\xa0]*\\/[\\s\\xa0]*2R[\\s\\xa0]*\\/[\\s\\xa0]*BX\\)[\\s\\xa0]*:*:[\\s\\xa0]*|Bronchiolite[\\s\\xa0]*lymphocytaire[\\s\\xa0]*\\(B[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\w]*)\"\n",
    "    bronchlymph__match = re.search(bronchlymph_pattern, text, re.DOTALL)\n",
    "    return bronchlymph__match.group(2).strip() if bronchlymph__match else None\n",
    "\n",
    "\n",
    "def extract_infl_lymph(text):\n",
    "    infl_lymph_pattern = r\"(Inflammation[\\s\\xa0]*lymphocytaire[\\s\\xa0]*bronchique[\\s\\xa0]*\\([\\s\\xa0]*oui[\\s\\xa0]*\\/[\\s\\xa0]*non[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\w]*)\"\n",
    "    infl_lymph_match = re.search(infl_lymph_pattern, text, re.DOTALL)\n",
    "    return infl_lymph_match.group(2).strip() if infl_lymph_match else None\n",
    "\n",
    "def extract_bronch_obliter(text):\n",
    "    bronch_obliter_pattern = r\"Bronchiolite[\\s\\xa0]*(oblitérante|constrictive)[\\s\\xa0]*:*[\\s\\xa0]*([\\w\\s]*)\"\n",
    "    bronch_obliter_match = re.search(bronch_obliter_pattern, text, re.DOTALL)\n",
    "    return bronch_obliter_match.group(2).strip() if bronch_obliter_match else None\n",
    "\n",
    "def extract_fibroelastose(text):  \n",
    "    fibroelastose_pattern = r\"(Fibro(?:-|\\s*)?élastose[\\s\\xa0]*interstitielle [\\s\\xa0]*\\([\\s\\xa0]*0[\\s\\xa0]*ou[\\s\\xa0]*1[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\w]*)\"\n",
    "    fibroelastose_match = re.search(fibroelastose_pattern, text, re.DOTALL)\n",
    "    return fibroelastose_match.group(2).strip() if fibroelastose_match else None\n",
    "\n",
    "\n",
    "def extract_PNN_cloisons(text):\n",
    "    pnn_pattern = r\"(PNN[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*cloisons[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*|PNN[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*cloisons[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([^\\n]+)\"\n",
    "    pnn_match = re.search(pnn_pattern, text, re.DOTALL)\n",
    "    return pnn_match.group(2).strip() if pnn_match else None\n",
    "\n",
    "\n",
    "def extract_cellmono(text):\n",
    "    cellmono_pattern = r\"(Cellules[\\s\\xa0]*mononucléées[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*capillaires[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*|Cellules[\\s\\xa0]*mononuclées[\\s\\xa0]*\\(lymphocytes[\\s\\xa0]*ou[\\s\\xa0]*macrophages[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*cloisons[\\s\\xa0]*alvéolaires\\)[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n]+)\"\n",
    "    cellmono_match = re.search(cellmono_pattern, text, re.DOTALL)\n",
    "    return cellmono_match.group(2).strip() if cellmono_match else None\n",
    "\n",
    "def extract_dilatation(text):\n",
    "    dilatation_pattern = r\"(Dilatation[\\s\\xa0]*des[\\s\\xa0]*capillaires[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n]+)\"\n",
    "    dilatation_match = re.search(dilatation_pattern, text, re.DOTALL)\n",
    "    return dilatation_match.group(2).strip() if dilatation_match else None\n",
    "\n",
    "def extract_oedeme(text):\n",
    "    oedeme_pattern = r\"((Œdème|Oedème)[\\s\\xa0]*des[\\s\\xa0]*cloisons[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    oedeme_match = re.search(oedeme_pattern, text, re.DOTALL)\n",
    "    return oedeme_match.group(3).strip() if oedeme_match else None\n",
    "\n",
    "def extract_hyperplasie(text):\n",
    "    hyperplasie_pattern = r\"(Hyperplasie[\\s\\xa0]*pneumocytaire[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    hyperplasie_match = re.search(hyperplasie_pattern, text, re.DOTALL)\n",
    "    return hyperplasie_match.group(2).strip() if hyperplasie_match else None\n",
    "\n",
    "def extract_PNN_espace(text):\n",
    "    pnn_pattern = r\"(PNN[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*espaces[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*|PNN[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*espaces[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([^\\n]+)\"\n",
    "    pnn_match = re.search(pnn_pattern, text, re.DOTALL)\n",
    "    return pnn_match.group(2).strip() if pnn_match else None\n",
    "\n",
    "def extract_thrombifib(text):\n",
    "    tbf_pattern = r\"(Thrombi[\\s\\xa0]*fibrineux[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*capillaires[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    tbf_match = re.search(tbf_pattern, text, re.DOTALL)\n",
    "    return tbf_match.group(2).strip() if tbf_match else None\n",
    "\n",
    "def extract_debriscell(text):\n",
    "    debris_pattern = r\"(Débris[\\s\\xa0]*cellulaires[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*cloisons[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    debris_match = re.search(debris_pattern, text, re.DOTALL)\n",
    "    return debris_match.group(2).strip() if debris_match else None\n",
    "\n",
    "def extract_epaissfibreux(text):\n",
    "    epaiss_pattern = r\"(Epaississement[\\s\\xa0]*fibreux[\\s\\xa0]*des[\\s\\xa0]*cloisons[\\s\\xa0]*alvéolaires[\\s\\xa0]*(?:\\w*)?\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    epaiss_match = re.search(epaiss_pattern, text, re.DOTALL)\n",
    "    return epaiss_match.group(2).strip() if epaiss_match else None\n",
    "\n",
    "def extract_macrophages(text):\n",
    "    pattern = r\"(Macrophages[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*espaces[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(2).strip() if match else None\n",
    "\n",
    "def extract_bourgeons(text):\n",
    "    pattern = r\"(Bourgeons[\\s\\xa0]*conjonctifs[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*espaces[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(2).strip() if match else None\n",
    "\n",
    "def extract_bourgeons(text):\n",
    "    pattern = r\"(Bourgeons[\\s\\xa0]*conjonctifs[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*espaces[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(2).strip() if match else None\n",
    "\n",
    "def extract_hematies(text):\n",
    "    pattern = r\"(Hématies[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*espaces[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(2).strip() if match else None\n",
    "\n",
    "def extract_membranes(text):\n",
    "    pattern = r\"(Membranes[\\s\\xa0]*hyalines[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(2).strip() if match else None\n",
    "\n",
    "def extract_fibrines(text):\n",
    "    pattern = r\"(Fibrine[\\s\\xa0]*dans[\\s\\xa0]*les[\\s\\xa0]*espaces[\\s\\xa0]*alvéolaires[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(2).strip() if match else None\n",
    "\n",
    "def extract_inflspsbb(text):\n",
    "    pattern = r\"(Inflammation[\\s\\xa0]*sous(\\-|[\\s\\xa0])?pleurale,[\\s\\xa0]*septale,[\\s\\xa0]*bronchique[\\s\\xa0]*ou[\\s\\xa0]*bronchiolaire[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(3).strip() if match else None \n",
    "\n",
    "def extract_inflspsbb(text):\n",
    "    pattern = r\"(Inflammation[\\s\\xa0]*sous(\\-|[\\s\\xa0])?pleurale,[\\s\\xa0]*septale,[\\s\\xa0]*bronchique[\\s\\xa0]*ou[\\s\\xa0]*bronchiolaire[\\s\\xa0]*\\(0[\\s\\xa0]*à[\\s\\xa0]*\\+\\+\\+[\\s\\xa0]*\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(3).strip() if match else None\n",
    "\n",
    "def extract_BALT(text):\n",
    "    pattern = r\"(BALT[\\s\\xa0]*\\(oui\\/non\\)[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(2).strip() if match else None\n",
    "\n",
    "def extract_thrombus(text): \n",
    "    pattern = r\"(Thrombus[\\s\\xa0]*fibrino(\\-|[\\s\\xa0]|)?cruorique[\\s\\xa0]*(\\(oui\\/non\\)|)?[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(4).strip() if match else None\n",
    "\n",
    "def extract_necrose(text): \n",
    "    pattern = r\"(Nécrose[\\s\\xa0]*ischémique[\\s\\xa0]*(\\(oui\\/non\\)|)?[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(3).strip() if match else None\n",
    "\n",
    "def extract_incluvirales(text): \n",
    "    pattern = r\"(Inclusions[\\s\\xa0]*virales[\\s\\xa0]*(\\(oui\\/non\\)|)?[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(3).strip() if match else None\n",
    "\n",
    "def extract_agentpatho(text):\n",
    "    pattern = r\"(Agent[\\s\\xa0]*pathogène[\\s\\xa0]*(\\(oui\\/non\\)|)?[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(3).strip() if match else None \n",
    "\n",
    "def extract_eosino(text):\n",
    "    pattern = r\"(Eosinophilie[\\s\\xa0]*(\\(interstitielle\\/alvéolaire\\)|)?[\\s\\xa0]*(\\(oui\\/non\\)|)?[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(4).strip() if match else None     \n",
    "\n",
    "def extract_remodelage_vasculaire(text):\n",
    "    pattern = r\"(Remodelage[\\s\\xa0]*vasculaire[\\s\\xa0]*[\\s\\xa0]*(\\(oui\\/non\\)|)?[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(3).strip() if match else None   \n",
    "\n",
    "def extract_mat_etranger_inhalation(text):\n",
    "    pattern = r\"(Matériel[\\s\\xa0]*étranger[\\s\\xa0]*d’inhalation[\\s\\xa0]*(\\(oui\\/non\\)|)?[\\s\\xa0]*:*:[\\s\\xa0]*)([\\S]*[^\\n])\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(3).strip() if match else None  \n",
    "\n",
    "def extract_conclusion(text):\n",
    "    pattern = r\"(Conclusion[\\s\\xa0]*(:.*|)?)([\\s\\S]*)\"\n",
    "    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "    return match.group(3).strip() if match else None  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_files_in_directory(directory_path):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            ipp = filename[:9]\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            text = read_text_file(file_path)\n",
    "            nom = extract_nom(text)\n",
    "            prenom = extract_prenom_before_docteur(text)\n",
    "            ddn = extract_ddn(text)\n",
    "            sexe = extract_sex(text)\n",
    "            date_prelevement = extract_ddprelevement(text)\n",
    "            technique = extract_technique(text)\n",
    "            niveaux = extract_niveaux_coupes(text)\n",
    "            site = extract_site(text)\n",
    "            frag_alv = extract_fragment_alveolaire(text)\n",
    "            bronche_bronchiole = extract_bronche_bronchiole(text)\n",
    "            infiltrat = extract_infiltrat(text)\n",
    "            bronch_lymph = extract_bronchiolite_lymph(text)\n",
    "            infl_lymph = extract_infl_lymph(text)\n",
    "            bronch_obliter = extract_bronch_obliter(text)\n",
    "            fibro_elastose = extract_fibroelastose(text)\n",
    "            pnn_cloisons = extract_PNN_cloisons(text)\n",
    "            cellmono = extract_cellmono(text)\n",
    "            dilatation = extract_dilatation(text)\n",
    "            oedeme = extract_oedeme(text)\n",
    "            hyperplasie = extract_hyperplasie(text)\n",
    "            pnn_espaces = extract_PNN_espace(text)\n",
    "            tbf = extract_thrombifib(text)\n",
    "            debris = extract_debriscell(text)\n",
    "            epaiss_fibreux = extract_epaissfibreux(text)\n",
    "            macrophages = extract_macrophages(text)\n",
    "            bourgeons = extract_bourgeons(text)\n",
    "            hematies = extract_hematies(text)\n",
    "            membranes = extract_membranes(text)\n",
    "            fibrines = extract_fibrines(text)\n",
    "            inflspbb = extract_inflspsbb(text)\n",
    "            balt = extract_BALT(text)\n",
    "            thrombus = extract_thrombus(text)\n",
    "            necrose = extract_necrose(text)\n",
    "            inclus_virale = extract_incluvirales(text)\n",
    "            agent_patho = extract_agentpatho(text)\n",
    "            eosino = extract_eosino(text)\n",
    "            remodelage_vasculaire = extract_remodelage_vasculaire(text)\n",
    "            mat_etranger_inhalation = extract_mat_etranger_inhalation(text)\n",
    "            conclusion = extract_conclusion(text)\n",
    "            data.append(\n",
    "                [\n",
    "                    filename,\n",
    "                    ipp,\n",
    "                    nom,\n",
    "                    prenom,\n",
    "                    ddn,\n",
    "                    sexe,\n",
    "                    date_prelevement,\n",
    "                    technique,\n",
    "                    niveaux,\n",
    "                    site,\n",
    "                    frag_alv,\n",
    "                    bronche_bronchiole,\n",
    "                    infiltrat,\n",
    "                    bronch_lymph,\n",
    "                    infl_lymph,\n",
    "                    bronch_obliter,\n",
    "                    fibro_elastose,\n",
    "                    pnn_cloisons,\n",
    "                    cellmono,\n",
    "                    dilatation,\n",
    "                    oedeme,\n",
    "                    tbf,\n",
    "                    debris,\n",
    "                    epaiss_fibreux,\n",
    "                    hyperplasie,\n",
    "                    pnn_espaces,\n",
    "                    macrophages,\n",
    "                    bourgeons,\n",
    "                    hematies,\n",
    "                    membranes,\n",
    "                    fibrines,\n",
    "                    inflspbb,\n",
    "                    balt,\n",
    "                    thrombus,\n",
    "                    necrose,\n",
    "                    inclus_virale,\n",
    "                    agent_patho,\n",
    "                    eosino,\n",
    "                    remodelage_vasculaire,\n",
    "                    mat_etranger_inhalation,\n",
    "                    conclusion\n",
    "                    \n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\n",
    "            \"Filename\",\n",
    "            \"IPP\",\n",
    "            \"Nom\",\n",
    "            \"Prénom\",\n",
    "            \"Date de naissance\",\n",
    "            \"Sexe\",\n",
    "            \"Date de Prélèvement\",\n",
    "            \"Technique\",\n",
    "            \"Niveaux de coupes\",\n",
    "            \"Site\",\n",
    "            \"Nombre de fragment alvéolaire\",\n",
    "            \"Bronches/Bronchioles\",\n",
    "            \"Infiltrat\",\n",
    "            \"Bronchiolite Lymphocytaire\",\n",
    "            \"Inflammation Lymphocytaire\",\n",
    "            \"Bronchiolite oblitérante\",\n",
    "            \"Fibro-élastose interstitielle\",\n",
    "            \"PNN dans les cloisons alvéolaires\",\n",
    "            \"Cellules mononucléées\",\n",
    "            \"Dilatation des capillaires alvéolaires\",\n",
    "            \"Œdème des cloisons alvéolaires\",\n",
    "            \"Thrombi fibrineux dans les capillaires alvéolaires\",\n",
    "            \"Débris cellulaires dans les cloisons alvéolaires\",\n",
    "            \"Epaississement fibreux des cloisons alvéolaires\",\n",
    "            \"Hyperplasie pneumocytaire\",\n",
    "            \"PNN dans les espaces alvéolaires\",\n",
    "            \"Macrophages dans les espaces alvéolaires\",\n",
    "            \"Bourgeons conjonctifs dans les espaces alvéolaires\",\n",
    "            \"Hématies dans les espaces alvéolaires\",\n",
    "            \"Membranes hyalines\",\n",
    "            \"Fibrine dans les espaces alvéolaires\",\n",
    "            \"Inflammation sous-pleurale, septale, bronchique ou bronchiolaire\",\n",
    "            \"BALT\",\n",
    "            \"Thrombus fibrino-cruorique\",\n",
    "            \"Nécrose ischémique\",\n",
    "            \"Inclusions virales\",\n",
    "            \"Agent pathogène\",\n",
    "            \"Eosinophilie (interstitielle/alvéolaire)\",\n",
    "            \"Remodelage vasculaire\",\n",
    "            \"Matériel étranger d’inhalation\",\n",
    "            \"Conclusion\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "# Replace 'path_to_your_directory_with_txts' with your directory's path\n",
    "directory_path = \"./data/data_extractMoustapha_2020:2021\"\n",
    "df = process_text_files_in_directory(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames are saved in 'data_summary.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "df['Date de Prélèvement'] = pd.to_datetime(df['Date de Prélèvement'])\n",
    "\n",
    "unique_values_dict = {}\n",
    "\n",
    "for column in df.columns:\n",
    "    if column in ['IPP','Filename','Nom','Prénom','Date de naissance','Date de Prélèvement','Conclusion']:\n",
    "        continue\n",
    "    unique_values = df[column].dropna().unique().tolist()\n",
    "    unique_values_dict[column] = unique_values\n",
    "max_len = max(len(v) for v in unique_values_dict.values()) \n",
    "for column in unique_values_dict.keys():\n",
    "    current_len = len(unique_values_dict[column])\n",
    "    if current_len < max_len:\n",
    "        unique_values_dict[column].extend([None] * (max_len - current_len))\n",
    "\n",
    "unique_values_df = pd.DataFrame(unique_values_dict)\n",
    "\n",
    "unique_values_counts = df.nunique()\n",
    "na_counts = df.isna().sum()\n",
    "df['Year'] = df['Date de Prélèvement'].dt.year\n",
    "na_counts_by_year = df.groupby('Year').apply(lambda x: x.isna().sum()).drop('Year', axis=1)\n",
    "\n",
    "with pd.ExcelWriter('data_summary.xlsx', engine='openpyxl', mode='w') as writer:\n",
    "    unique_values_counts.to_frame(name='Unique Values Count').to_excel(writer, sheet_name='Unique Values Counts')\n",
    "    na_counts.to_frame(name='NA Counts').to_excel(writer, sheet_name='NA Counts')\n",
    "    na_counts_by_year.to_excel(writer, sheet_name='NA Counts by Year')\n",
    "    unique_values_df.to_excel(writer, sheet_name='Unique Values')\n",
    "\n",
    "print(\"DataFrames are saved in 'data_summary.xlsx'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
